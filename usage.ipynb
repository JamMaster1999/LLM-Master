{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import sys\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.response_synthesizer:Initialized QueryLLM handler\n"
     ]
    }
   ],
   "source": [
    "import time, base64, json, requests, asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import List, Dict, Union, Any \n",
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', force=True)     \n",
    "# Import our providers\n",
    "from llm_master import QueryLLM, LLMConfig\n",
    "config = LLMConfig.from_env()\n",
    "llm = QueryLLM(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.openai_provider:Successfully initialized OpenAI provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text response:\n",
      "Instantiating provider: openai_provider with class OpenAIProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```py\n",
      "import docx\n",
      "doc = docx.Document()\n",
      "doc.add_paragraph(\"Test\")\n",
      "doc.save(\"/mnt/data/test.docx\")\n",
      "\"/mnt/data/test.docx exists\"\n",
      "```\n",
      "\n",
      "\n",
      "```plaintext\n",
      "'/mnt/data/test.docx exists'\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<think>\n",
      "\n",
      "**Generating docx file**\n",
      "\n",
      "I'm excited to generate a docx file for the user! I need to run code to create the final document, but since the output won‚Äôt be visible to the user until they see the link, that's okay. I‚Äôll prepare the lesson plan, which will include the title, age group, objectives, materials, introduction, and main activity. Once the file is created, I‚Äôll provide the link using markdown for easy access. Let's get started on writing the code!**Creating lesson plan document**\n",
      "\n",
      "I‚Äôm focusing on preparing a lesson plan about the importance of water. I want to include simple facts like how water keeps us alive and helps plants grow. I'll add some discussion questions, a recap for the conclusion, and an extension activity involving drawing water habits and discussing conservation. Once I've written the necessary paragraphs and headings, I'll save the document. Let‚Äôs get to work on generating the docx file for the user!\n",
      "```py\n",
      "from docx import Document\n",
      "\n",
      "doc = Document()\n",
      "doc.add_heading('Lesson Plan: The Importance of Water', level=1)\n",
      "\n",
      "# Basic info\n",
      "doc.add_paragraph('Age Group: 5-year-old children')\n",
      "doc.add_paragraph('Duration: 30 minutes')\n",
      "doc.add_paragraph('Objective:')\n",
      "doc.add_paragraph('  ‚Ä¢ Understand what water is', style='List Bullet')\n",
      "doc.add_paragraph('  ‚Ä¢ Recognize why water is important for life', style='List Bullet')\n",
      "doc.add_paragraph('  ‚Ä¢ Identify ways to use and conserve water', style='List Bullet')\n",
      "\n",
      "# Materials\n",
      "doc.add_heading('Materials', level=2)\n",
      "doc.add_paragraph('- A clear glass of water')\n",
      "doc.add_paragraph('- Small bowls or containers')\n",
      "doc.add_paragraph('- Coloring (food coloring or washable paint)')\n",
      "doc.add_paragraph('- Picture book or images of water in nature')\n",
      "doc.add_paragraph('- Cups and spoons for pouring activity')\n",
      "\n",
      "# Introduction\n",
      "doc.add_heading('Introduction (5 minutes)', level=2)\n",
      "doc.add_paragraph('1. Greet the child and show them a glass of water.')\n",
      "doc.add_paragraph('2. Ask: \"What do you think this is?\" and listen to their answers.')\n",
      "doc.add_paragraph('3. Explain: \"This is water. Everyone and everything needs water to live.\"')\n",
      "\n",
      "# Main Activity\n",
      "doc.add_heading('Main Activity (15 minutes)', level=2)\n",
      "doc.add_paragraph('Activity 1: Exploring Water Sensory Play')\n",
      "doc.add_paragraph('  ‚Ä¢ Let the child pour water between bowls with cups and spoons.')\n",
      "doc.add_paragraph('  ‚Ä¢ Ask them how the water feels. Is it cold? Smooth?')\n",
      "doc.add_paragraph('Activity 2: Coloring Water')\n",
      "doc.add_paragraph('  ‚Ä¢ Add a drop of food coloring to water and watch it change color.')\n",
      "doc.add_paragraph('  ‚Ä¢ Talk about how water can mix with things to change.')\n",
      "\n",
      "# Discussion\n",
      "doc.add_heading('Discussion (5 minutes)', level=2)\n",
      "doc.add_paragraph('Ask questions:')\n",
      "doc.add_paragraph('  ‚Ä¢ Why do you think we need to drink water?')\n",
      "doc.add_paragraph('  ‚Ä¢ What happens if we do not have water to drink?')\n",
      "doc.add_paragraph('  ‚Ä¢ Can you think of other things that use water? (Plants, animals)')\n",
      "\n",
      "# Conclusion\n",
      "doc.add_heading('Conclusion (5 minutes)', level=2)\n",
      "doc.add_paragraph('1. Summarize key points: water helps us stay healthy, helps plants grow, and we can play with it.')\n",
      "doc.add_paragraph('2. Encourage the child to drink a glass of water now.')\n",
      "doc.add_paragraph('3. Praise their observations and answers.')\n",
      "\n",
      "# Extension Activity (Optional)\n",
      "doc.add_heading('Extension Activities', level=2)\n",
      "doc.add_paragraph('- Draw a picture of something that needs water (e.g., a plant, fish, you drinking).')\n",
      "doc.add_paragraph('- Go on a \"water walk\": look for water at home or outside (sink, garden).')\n",
      "\n",
      "# Save document\n",
      "file_path = '/mnt/data/lesson_plan_the_importance_of_water.docx'\n",
      "doc.save(file_path)\n",
      "file_path\n",
      "```\n",
      "\n",
      "\n",
      "```plaintext\n",
      "'/mnt/data/lesson_plan_the_importance_of_water.docx'\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "</think>\n",
      "\n",
      "I‚Äôve created the lesson plan as a Word document. You can download it here:\n",
      "\n",
      "[Download the lesson plan: The Importance of Water (5-year-olds)](sandbox:/mnt/data/lesson_plan_the_importance_of_water.docx)\n",
      "\n",
      "<code_execution_file><code_execution_file_id>cfile_68e9be5e56b88191a271c96145841d93</code_execution_file_id><code_execution_file_name>lesson_plan_the_importance_of_water.docx</code_execution_file_name><code_execution_container_id>cntr_68e9be4af6a081918db5de5fddc0c87a00b9f9fec945e8db</code_execution_container_id></code_execution_file>\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "async def run_code_interpreter():\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response_generator = await llm.query(\n",
    "            model_name=\"responses-o4-mini\",\n",
    "            messages=[],\n",
    "            stream=True,\n",
    "            tools=[{\n",
    "                \"type\": \"code_interpreter\",\n",
    "                \"container\": {\n",
    "                    \"type\": \"auto\",\n",
    "                    # \"file_ids\": [\"file-TXT3RH5yycr7MAX2H8kLvq\"]\n",
    "                }\n",
    "            }],\n",
    "            input=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{ \"type\": \"input_text\", \"text\": \"Can you make a simple word document for lesson plan on the topic of 'The Importance of Water' for a 5 year old child? Please ensure you output a word document. You must include the file you generate in the annotation of the output text using a markdown url link in this format as an example: sandbox:/mnt/data/int100.txt\" }]\n",
    "            }],\n",
    "            reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "            text={\"format\": {\"type\": \"text\"}},\n",
    "            include=[\"code_interpreter_call.outputs\"],\n",
    "            max_output_tokens=32000\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_code_interpreter()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"type\":\"response.output_item.done\",\"sequence_number\":877,\"output_index\":2,\"item\":{\"id\":\"msg_6838c205884c8191a9ef3e5e8c5378170a1145d46963df26\",\"type\":\"message\",\"status\":\"completed\",\"content\":[{\"type\":\"output_text\",\"annotations\":[{\"type\":\"container_file_citation\",\"container_id\":\"cntr_6838c1f76ff88191b27d1dcd064592550c88636f8949b334\",\"end_index\":219,\"file_id\":\"cfile_6838c20627bc81919fbf3bae04fa20e2\",\"filename\":\"lesson_plan_importance_of_water.docx\",\"start_index\":165}],\"text\":\"I‚Äôve created the lesson plan document for \\\"The Importance of Water\\\" suitable for a 5-year-old. You can download it here:\\n\\n[Download the lesson plan (Word document)](sandbox:/mnt/data/lesson_plan_importance_of_water.docx)\"}],\"role\":\"assistant\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.response_synthesizer:Instantiating provider: openai_provider with class OpenAIProvider\n",
      "INFO:llm_master.openai_provider:Successfully initialized OpenAI provider\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1**  \n",
      "The setting in the first frame (the bar and line chart) is analytical and data-driven, taking place in a context where information is being compared and evaluated. The dominant mood is investigative and comparative, highlighted by the dual axes (Population and GDP Growth), which imply a search for relationships or patterns among Latin American countries. The viewer is invited to consider both economic and demographic dimensions, creating a sense of curiosity and focus.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 2**  \n",
      "Compared to the first frame, the second image (pie chart) shifts both camera angle and the ‚Äúsubject‚Äôs posture.‚Äù Here, the perspective changes from a linear, comparative format to a more centralized, holistic one. The camera metaphorically ‚Äúzooms out,‚Äù offering a top-down view that unifies the subject‚Äîpopulation‚Äîinto a single, cohesive whole rather than segmenting it with separate variables. The posture of the data is less about direct comparison and more about proportion and relationship, emphasizing the dominance of Brazil and Mexico within the population, and presenting a clearer sense of scale among the countries. This provides a visual mood that is more synthesizing and integrative.\n",
      "\n",
      "---\n",
      "\n",
      "**Step 3**  \n",
      "Reflecting on these shifts, the final image can be predicted to capture an emotional beat of resolution and clarity. The bar chart in the last image returns to a single-variable focus (Population), but the expansion to more countries and the lack of any overlaid line or secondary axis delivers a sense of simplicity and directness. The viewer experiences a form of visual relief and understanding after the complexity of the earlier frames. The emotional beat is one of conclusion and comprehension‚Äîthe puzzle pieces have been separated, examined, and now the viewer is left with an unambiguous, clear picture of the population landscape in Latin America."
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "async def run_stream():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [\n",
    "                \"You are a visual narrative analyst helping me study a storyboard sequence.\",\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Step 1 ‚Äì describe the setting and dominant mood in the first frame before you see anything else.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"url\": \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/ca106aaf-bfa3-45b9-bf70-eb612bbe27d0.png\",\n",
    "                    \"detail\": \"high\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Step 2 ‚Äì compare that first frame with the second image, focusing specifically on the change in camera angle and the subject‚Äôs posture.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"url\": \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/00b5fe2d-ff07-43e1-8f2e-ab73906a30b7.png\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Step 3 ‚Äì after reflecting on that comparison, use it to predict the emotional beat captured in the final image.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"url\": \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/2cc690db-8cc4-4737-8e97-ac96b4256793.png\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Deliver the answer with three sections titled Step 1, Step 2, and Step 3 so I can verify you tracked the interleaved instructions correctly.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    try:\n",
    "        response_generator = await llm.query(\n",
    "            model_name=\"responses-gpt-4.1\",\n",
    "            # reasoning={\"thinking_budget\": 0},\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            moderation=False,\n",
    "        )\n",
    "\n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "await run_stream()  # Don't use asyncio.run() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.response_synthesizer:Instantiating provider: google_genai with class GoogleGenAIProvider\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency to first token: 1381.56 ms\n",
      "\n",
      "Barnaby, a scruffy terrier mix, lunged at the tabby perched precariously on the fence, only to have the feline's tail flick in his face as she leapt gracefully onto the shed roof. Later that afternoon, they napped in a sunbeam, the dog's chin resting on the cat's fluffy tail, a truce called until the next squirrel or tempting bird appeared. \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "\n",
    "async def run_stream():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a two sentence story about a cat and a dog\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        first_chunk_time = None\n",
    "        \n",
    "        response_generator = await llm.query(\n",
    "            model_name=\"googleai:gemini-2.5-flash\",\n",
    "            reasoning={\"thinking_budget\": 0},\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            moderation=False,\n",
    "        )\n",
    "\n",
    "        i = 0\n",
    "        async for chunk in response_generator:\n",
    "            if i == 0:\n",
    "                first_chunk_time = time.time()\n",
    "                latency_ms = (first_chunk_time - start_time) * 1000\n",
    "                print(f\"Latency to first token: {latency_ms:.2f} ms\\n\")\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            i += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "await run_stream()  # Don't use asyncio.run() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency to first token: 431.00 ms\n",
      "\n",
      "Whiskers the cat watched Fluffy the dog chase a squirrel, her tail swishing with a familiar mix of annoyance and quiet contentment at their shared, if differently experienced, afternoon. Later, they both napped in a sunbeam, Fluffy's paw resting near Whiskers' twitching ear, a silent truce to their daily theatrics. "
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "start_time = time.time()\n",
    "first_chunk_time = None\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking\n",
    "    ),\n",
    "    contents=[\"Write a two sentence story about a cat and a dog\"]\n",
    ")\n",
    "for i, chunk in enumerate(response):\n",
    "    if i == 0:\n",
    "        first_chunk_time = time.time()\n",
    "        latency_ms = (first_chunk_time - start_time) * 1000\n",
    "        print(f\"Latency to first token: {latency_ms:.2f} ms\\n\")\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a regular computer is like a light switch. It can be either **on (1)** or **off (0)**. These are called **bits**.\n",
      "\n",
      "Quantum computing is fundamentally different. Instead of bits, it uses **qubits**. Here's how to think about them in simple terms:\n",
      "\n",
      "1.  **Qubits: Not Just On or Off**\n",
      "    *   **Classical Bit:** A light switch (either 0 or 1).\n",
      "    *   **Quantum Qubit:** Imagine a spinning coin. While it's spinning, it's not yet heads *or* tails. It's a bit of both! A qubit can be **0, 1, or both 0 and 1 at the same time** (this is called **superposition**).\n",
      "    *   **Why it matters:** This means a single qubit can hold much more information than a classical bit. If you have many qubits, they can explore many, many possibilities simultaneously.\n",
      "\n",
      "2.  **Superposition: The \"Spinning Coin\" State**\n",
      "    *   Because a qubit can be both 0 and 1 at the same time, a quantum computer can perform calculations on many possibilities concurrently. Instead of trying one path at a time, it can explore all paths simultaneously.\n",
      "    *   Think of it as parallel universes for computation!\n",
      "\n",
      "3.  **Entanglement: Spooky Connection**\n",
      "    *   Imagine you have two of these spinning coins, and you magically link them. Now, no matter how far apart they are, if one lands on heads, you instantly know the other will land on tails (or vice versa, depending on how they were linked).\n",
      "    *   **Entanglement** means two or more qubits become linked in such a way that they share the same fate, even if physically separated. Measuring one instantly tells you something about the others.\n",
      "    *   **Why it matters:** This allows qubits to work together in a highly coordinated way, sharing information that classical bits cannot, leading to incredibly complex computational relationships.\n",
      "\n",
      "4.  **Interference: Finding the Right Answer**\n",
      "    *   After exploring all these possibilities using superposition and entanglement, a quantum computer uses **interference**. This is like creating ripples in a pond. Correct answers cause ripples that reinforce each other, making them stronger, while incorrect answers cause ripples that cancel each other out.\n",
      "    *   The quantum computer is designed to increase the probability of getting the right answer and decrease the probability of getting the wrong ones.\n",
      "\n",
      "**How does it work, then?**\n",
      "\n",
      "1.  You set up the qubits in their initial spinning (superposition) states.\n",
      "2.  You manipulate them with \"quantum gates\" (like logical operations in classical computers) to make them interact and entangle.\n",
      "3.  The system explores countless solutions at once.\n",
      "4.  Through interference, the \"wrong\" answers cancel out, and the \"right\" answers get amplified.\n",
      "5.  Finally, you \"look\" at the qubits. When you look, their superposition collapses, and they settle into a definite 0 or 1, giving you the answer.\n",
      "\n",
      "**What is it good for?**\n",
      "\n",
      "Quantum computers are not meant to replace your laptop or smartphone for everyday tasks. They are designed to solve very specific, incredibly complex problems that are practically impossible for even the fastest classical supercomputers.\n",
      "\n",
      "*   **Drug Discovery & Materials Science:** Simulating molecules and chemical reactions at a fundamental level to design new drugs or materials with specific properties.\n",
      "*   **Cryptography:** Potentially breaking current encryption methods (and creating new, unbreakable ones).\n",
      "*   **Optimization:** Finding the absolute best solution from a vast number of possibilities (e.g., logistics, financial modeling, traffic flow).\n",
      "*   **Artificial Intelligence:** Enhancing machine learning algorithms.\n",
      "\n",
      "In essence, quantum computing uses the strange and powerful rules of quantum mechanics (like things being in multiple places at once) to process information in a fundamentally new way, allowing it to tackle problems that are currently beyond our reach.\n",
      "Imagine a regular computer is like a light switch. It can be either **on (1)** or **off (0)**. These are called **bits**.\n",
      "\n",
      "Quantum computing is fundamentally different. Instead of bits, it uses **qubits**. Here's how to think about them in simple terms:\n",
      "\n",
      "1.  **Qubits: Not Just On or Off**\n",
      "    *   **Classical Bit:** A light switch (either 0 or 1).\n",
      "    *   **Quantum Qubit:** Imagine a spinning coin. While it's spinning, it's not yet heads *or* tails. It's a bit of both! A qubit can be **0, 1, or both 0 and 1 at the same time** (this is called **superposition**).\n",
      "    *   **Why it matters:** This means a single qubit can hold much more information than a classical bit. If you have many qubits, they can explore many, many possibilities simultaneously.\n",
      "\n",
      "2.  **Superposition: The \"Spinning Coin\" State**\n",
      "    *   Because a qubit can be both 0 and 1 at the same time, a quantum computer can perform calculations on many possibilities concurrently. Instead of trying one path at a time, it can explore all paths simultaneously.\n",
      "    *   Think of it as parallel universes for computation!\n",
      "\n",
      "3.  **Entanglement: Spooky Connection**\n",
      "    *   Imagine you have two of these spinning coins, and you magically link them. Now, no matter how far apart they are, if one lands on heads, you instantly know the other will land on tails (or vice versa, depending on how they were linked).\n",
      "    *   **Entanglement** means two or more qubits become linked in such a way that they share the same fate, even if physically separated. Measuring one instantly tells you something about the others.\n",
      "    *   **Why it matters:** This allows qubits to work together in a highly coordinated way, sharing information that classical bits cannot, leading to incredibly complex computational relationships.\n",
      "\n",
      "4.  **Interference: Finding the Right Answer**\n",
      "    *   After exploring all these possibilities using superposition and entanglement, a quantum computer uses **interference**. This is like creating ripples in a pond. Correct answers cause ripples that reinforce each other, making them stronger, while incorrect answers cause ripples that cancel each other out.\n",
      "    *   The quantum computer is designed to increase the probability of getting the right answer and decrease the probability of getting the wrong ones.\n",
      "\n",
      "**How does it work, then?**\n",
      "\n",
      "1.  You set up the qubits in their initial spinning (superposition) states.\n",
      "2.  You manipulate them with \"quantum gates\" (like logical operations in classical computers) to make them interact and entangle.\n",
      "3.  The system explores countless solutions at once.\n",
      "4.  Through interference, the \"wrong\" answers cancel out, and the \"right\" answers get amplified.\n",
      "5.  Finally, you \"look\" at the qubits. When you look, their superposition collapses, and they settle into a definite 0 or 1, giving you the answer.\n",
      "\n",
      "**What is it good for?**\n",
      "\n",
      "Quantum computers are not meant to replace your laptop or smartphone for everyday tasks. They are designed to solve very specific, incredibly complex problems that are practically impossible for even the fastest classical supercomputers.\n",
      "\n",
      "*   **Drug Discovery & Materials Science:** Simulating molecules and chemical reactions at a fundamental level to design new drugs or materials with specific properties.\n",
      "*   **Cryptography:** Potentially breaking current encryption methods (and creating new, unbreakable ones).\n",
      "*   **Optimization:** Finding the absolute best solution from a vast number of possibilities (e.g., logistics, financial modeling, traffic flow).\n",
      "*   **Artificial Intelligence:** Enhancing machine learning algorithms.\n",
      "\n",
      "In essence, quantum computing uses the strange and powerful rules of quantum mechanics (like things being in multiple places at once) to process information in a fundamentally new way, allowing it to tackle problems that are currently beyond our reach.\n",
      "\n",
      "--- Response Metadata ---\n",
      "Model: googleai:gemini-2.5-flash\n",
      "Input tokens: 7\n",
      "Output tokens: 1873\n",
      "Cost: $0.004685\n",
      "Latency: 10.22 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            # model_name=\"accounts/fireworks/models/deepseek-r1\",\n",
    "            model_name=\"googleai:gemini-2.5-flash\",\n",
    "            # reasoning_effort=\"low\",\n",
    "            messages=messages,\n",
    "            stream=False,  # Set to False for non-streaming\n",
    "            # temperature=0.5,\n",
    "            fallback_provider=\"openai\",\n",
    "            fallback_model=\"gpt-4o\",\n",
    "            moderation=False\n",
    "        )\n",
    "        \n",
    "        # Print the full response content\n",
    "        print(response.content)\n",
    "        \n",
    "        # You can also access other metadata\n",
    "        print(\"\\n--- Response Metadata ---\")\n",
    "        print(f\"Model: {response.model_name}\")\n",
    "        print(f\"Input tokens: {response.usage.input_tokens}\")\n",
    "        print(f\"Output tokens: {response.usage.output_tokens}\")\n",
    "        print(f\"Cost: ${response.cost:.6f}\")\n",
    "        print(f\"Latency: {response.latency:.2f} seconds\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def test_perplexity():\n",
    "    # Sample messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an artificial intelligence assistant and you need to engage in a helpful, detailed, polite conversation with a user. Answer as concisely as possible.\"\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many stars are in the universe?\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Test non-streaming query\n",
    "    print(\"Testing non-streaming Perplexity API with citations\")\n",
    "    response = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=False,\n",
    "        extra_body={\n",
    "            \"return_images\": True,\n",
    "            \"web_search_options\": {\n",
    "                \"search_context_size\": \"low\"\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    print(f\"Response content: {response.content}\")\n",
    "    # print(f\"Citations: {response.citations}\")\n",
    "    \n",
    "    # Test streaming query\n",
    "    print(\"\\nTesting streaming Perplexity API with citations\")\n",
    "    stream_generator = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    citations_found = False\n",
    "    \n",
    "    async for chunk in stream_generator:\n",
    "        if chunk.startswith(\"\\n<citations>\"):\n",
    "            print(f\"Found citations in stream: {chunk}\")\n",
    "            citations_found = True\n",
    "        else:\n",
    "            full_response += chunk\n",
    "            print(f\"Received chunk: {chunk}\")\n",
    "    \n",
    "    print(f\"\\nFull response: {full_response[:100]}...\")\n",
    "    \n",
    "    # After streaming is complete, check if provider has citations\n",
    "    provider = llm._get_provider(\"sonar\")\n",
    "    if hasattr(provider, 'last_citations') and provider.last_citations:\n",
    "        print(f\"Citations from provider.last_citations: {provider.last_citations}\")\n",
    "        citations_found = True\n",
    "    \n",
    "    if not citations_found:\n",
    "        print(\"No citations found in streaming response\")\n",
    "\n",
    "await test_perplexity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"A cat on its back legs running like a human is holding a big silver fish with its arms. The cat is running away from the shop owner and has a panicked look on his face. The scene is situated in a crowded market.\"\n",
    "\n",
    "try:\n",
    "    # Generate the image\n",
    "    response = await llm.query(\n",
    "        model_name=\"recraftv3\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        style=\"digital_illustration\"\n",
    "    )\n",
    "    \n",
    "    # Print the result (which is the image URL)\n",
    "    print(f\"Image generation successful! URL: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.response_synthesizer:Instantiating provider: openai with class UnifiedProvider\n",
      "INFO:llm_master.base_provider:Initialized openai provider with base URL: None\n",
      "INFO:llm_master.base_provider:Successfully initialized openai provider\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text response:\n",
      "None\n",
      "\n",
      "Audio saved to 'dog_response.wav'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Narrate the text: \"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../chat/prompts/roleplay_prompts.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå No cache hits detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m test_gemini_caching_with_long_prompt()\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mtest_gemini_caching_with_long_prompt\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_gemini_caching_with_long_prompt\u001b[39m():\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Test Gemini caching with a very long prompt from desmos documentation\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../chat/prompts/roleplay_prompts.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m         roleplay_prompts \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      8\u001b[0m     socratic_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m roleplay_prompts \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocratic\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Experiments/ingest-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../chat/prompts/roleplay_prompts.yaml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "MODEL_NAME = \"vertexai:gemini-2.5-pro\"\n",
    "async def test_gemini_caching_with_long_prompt():\n",
    "    \"\"\"Test Gemini caching with a very long prompt from desmos documentation\"\"\"\n",
    "    with open(\"../chat/prompts/roleplay_prompts.yaml\", \"r\") as f:\n",
    "        roleplay_prompts = yaml.safe_load(f.read())\n",
    "\n",
    "    socratic_prompt = next((item for item in roleplay_prompts if item.get(\"type\") == \"socratic\"), None)\n",
    "\n",
    "    # Create a long prompt with the documentation\n",
    "    long_prompt = f\"\"\"Here is the complete Desmos API documentation:\n",
    "\n",
    "{socratic_prompt.get(\"prompt\")}\n",
    "\n",
    "Based on this documentation, please answer this simple question: What is this text about? Give me a brief 2-3 sentence summary.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": long_prompt}]\n",
    "    \n",
    "    print(\"Testing Gemini caching with long prompt...\")\n",
    "    print(f\"Prompt length: {len(long_prompt)} characters\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test 1: First call (should not have cache hits)\n",
    "    print(\"\\nüî• FIRST CALL (no cache):\")\n",
    "    response1 = await llm.query(\n",
    "        model_name=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"Response: {response1.content}\")\n",
    "    print(f\"Usage: {response1.usage}\")\n",
    "    print(f\"Cost: ${response1.cost:.6f}\")\n",
    "    time.sleep(10)\n",
    "    # Test 2: Second call with previous response + new question (should have cache hits if caching works)\n",
    "    print(\"\\n‚ôªÔ∏è  SECOND CALL (with previous response + new question):\")\n",
    "    \n",
    "    # Create new messages including the previous response and a new question\n",
    "    messages_with_response = [\n",
    "        {\"role\": \"user\", \"content\": long_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response1.content},\n",
    "        {\"role\": \"user\", \"content\": \"Based on your summary, can you now tell me what are the 3 most important features of a good socratic conversation?\"}\n",
    "    ]\n",
    "    \n",
    "    response2 = await llm.query(\n",
    "        model_name=MODEL_NAME, \n",
    "        messages=messages_with_response,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"Response: {response2.content}\")\n",
    "    print(f\"Usage: {response2.usage}\")\n",
    "    print(f\"Cost: ${response2.cost:.6f}\")\n",
    "    \n",
    "    # Test 3: Streaming version with same conversation context\n",
    "    print(\"\\nüåä STREAMING CALL (with conversation context):\")\n",
    "    messages_with_response2 = [\n",
    "        {\"role\": \"user\", \"content\": long_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response1.content},\n",
    "        {\"role\": \"user\", \"content\": \"Based on your summary, can you now tell me what are the 3 most important features of a good socratic conversation?\"},\n",
    "        {\"role\": \"assistant\", \"content\": response2.content},\n",
    "        {\"role\": \"user\", \"content\": \"What is the most difficult part of a socratic conversation in your opinion?\"}\n",
    "    ]\n",
    "    stream = await llm.query(\n",
    "        model_name=MODEL_NAME,\n",
    "        messages=messages_with_response2, \n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_content = \"\"\n",
    "    async for chunk in stream:\n",
    "        full_content += chunk\n",
    "    \n",
    "    provider = llm._get_provider(MODEL_NAME)\n",
    "    if hasattr(provider, 'last_usage') and provider.last_usage:\n",
    "        print(f\"Streamed response: {full_content}\")\n",
    "        print(f\"Usage: {provider.last_usage}\")\n",
    "        try:\n",
    "            from llm_master.classes import ModelRegistry\n",
    "            model_config = ModelRegistry.get_config(MODEL_NAME)\n",
    "            cost = provider.last_usage.calculate_cost(model_config)\n",
    "            print(f\"Cost: ${cost:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cost calculation failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üîç ANALYSIS:\")\n",
    "    print(f\"Call 1 tokens: {response1.usage.input_tokens} input, {response1.usage.output_tokens} output, {response1.usage.cached_tokens} cached\")\n",
    "    print(f\"Call 2 tokens: {response2.usage.input_tokens} input, {response2.usage.output_tokens} output, {response2.usage.cached_tokens} cached\")\n",
    "    if hasattr(provider, 'last_usage') and provider.last_usage:\n",
    "        print(f\"Stream tokens: {provider.last_usage.input_tokens} input, {provider.last_usage.output_tokens} output, {provider.last_usage.cached_tokens} cached\")\n",
    "    \n",
    "    if response2.usage.cached_tokens > 0:\n",
    "        print(\"üéâ CACHING IS WORKING!\")\n",
    "    else:\n",
    "        print(\"‚ùå No cache hits detected\")\n",
    "\n",
    "# Run the test\n",
    "await test_gemini_caching_with_long_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o3\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"search_context_size\": \"low\",\n",
    "    }],\n",
    "    input=\"What movie won best picture in 2025?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingest-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
