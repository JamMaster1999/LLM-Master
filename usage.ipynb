{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import sys\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sina/Desktop/Uflo Platform/uflo-AI-server/ai-server/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:llm_master.response_synthesizer:Initialized QueryLLM handler with rate limiters\n"
     ]
    }
   ],
   "source": [
    "import time, base64, json, requests, asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import List, Dict, Union, Any \n",
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', force=True)     \n",
    "# Import our providers\n",
    "from llm_master import QueryLLM, LLMConfig\n",
    "config = LLMConfig.from_env()\n",
    "llm = QueryLLM(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.openai_provider:Successfully initialized OpenAI provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text response:\n",
      "Instantiating provider: openai_provider with class OpenAIProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```py\n",
      "from docx import Document\n",
      "\n",
      "# Create a new Word document\n",
      "doc = Document()\n",
      "\n",
      "# Add title and basic information\n",
      "doc.add_heading(\"Lesson Plan: The Importance of Water\", level=1)\n",
      "doc.add_paragraph(\"Age Group: 5 year olds\")\n",
      "doc.add_paragraph(\"Duration: 30 minutes\")\n",
      "\n",
      "# Add objectives\n",
      "doc.add_heading(\"Objectives\", level=2)\n",
      "objectives = [\n",
      "    \"Understand why water is important for life\",\n",
      "    \"Identify sources of water\",\n",
      "    \"Practice good water use habits\"\n",
      "]\n",
      "for obj in objectives:\n",
      "    doc.add_paragraph(f\"• {obj}\")\n",
      "\n",
      "# Add materials\n",
      "doc.add_heading(\"Materials\", level=2)\n",
      "materials = [\n",
      "    \"A glass of water\",\n",
      "    \"Pictures of water uses (plants, animals, drinking)\",\n",
      "    \"Coloring sheets with water drops\",\n",
      "    \"Crayons\"\n",
      "]\n",
      "for item in materials:\n",
      "    doc.add_paragraph(f\"• {item}\")\n",
      "\n",
      "# Add lesson outline\n",
      "doc.add_heading(\"Lesson Outline\", level=2)\n",
      "\n",
      "# Introduction\n",
      "doc.add_heading(\"1. Introduction (5 min)\", level=3)\n",
      "intro_steps = [\n",
      "    \"Greet the children.\",\n",
      "    \"Show a glass of water; ask what it is.\"\n",
      "]\n",
      "for step in intro_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Story Time\n",
      "doc.add_heading(\"2. Story Time (5 min)\", level=3)\n",
      "story_steps = [\n",
      "    \"Read a short story about a thirsty plant or animal.\"\n",
      "]\n",
      "for step in story_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Discussion\n",
      "doc.add_heading(\"3. Discussion (5 min)\", level=3)\n",
      "discussion_steps = [\n",
      "    \"Ask: Why do we need water?\",\n",
      "    \"Discuss how plants, animals, and people use water.\"\n",
      "]\n",
      "for step in discussion_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Activity\n",
      "doc.add_heading(\"4. Activity (10 min)\", level=3)\n",
      "activity_steps = [\n",
      "    \"Coloring activity: coloring water drops and pictures.\"\n",
      "]\n",
      "for step in activity_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Practical Demo\n",
      "doc.add_heading(\"5. Practical Demo (5 min)\", level=3)\n",
      "demo_steps = [\n",
      "    \"Show a small plant and water it to demonstrate how plants drink water.\"\n",
      "]\n",
      "for step in demo_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Conclusion and Assessment\n",
      "doc.add_heading(\"Conclusion and Assessment\", level=2)\n",
      "doc.add_paragraph(\"Recap key points: water helps us grow, water plants, animals, and ourselves.\")\n",
      "doc.add_paragraph(\"Sing a short water song together.\")\n",
      "doc.add_paragraph(\"Ask each child to name one way they use water.\")\n",
      "\n",
      "# Extensions\n",
      "doc.add_heading(\"Extensions\", level=2)\n",
      "extensions = [\n",
      "    \"Teach simple water-saving tips: turn off the tap while brushing teeth.\",\n",
      "    \"Water a class plant daily and observe growth.\"\n",
      "]\n",
      "for ext in extensions:\n",
      "    doc.add_paragraph(f\"• {ext}\")\n",
      "\n",
      "# Save the document\n",
      "file_path = \"/mnt/data/importance_of_water_lesson_plan.docx\"\n",
      "doc.save(file_path)\n",
      "\n",
      "file_path\n",
      "```\n",
      "\n",
      "```plaintext\n",
      "'/mnt/data/importance_of_water_lesson_plan.docx'\n",
      "```\n",
      "\n",
      "Here is the lesson plan on \"The Importance of Water\" for 5-year-old children. You can download the Word document using the link below:\n",
      "\n",
      "[Download Lesson Plan (Importance_of_Water)](sandbox:/mnt/data/importance_of_water_lesson_plan.docx)<code_execution_file><code_execution_file_id>cfile_6838bbecaa608191a7298200cf3c8eaf</code_execution_file_id><code_execution_file_name>importance_of_water_lesson_plan.docx</code_execution_file_name></code_execution_file>\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "async def run_code_interpreter():\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response_generator = await llm.query(\n",
    "            model_name=\"responses-o4-mini\",\n",
    "            messages=[],\n",
    "            stream=True,\n",
    "            tools=[{\n",
    "                \"type\": \"code_interpreter\",\n",
    "                \"container\": {\n",
    "                    \"type\": \"auto\",\n",
    "                    # \"file_ids\": [\"file-TXT3RH5yycr7MAX2H8kLvq\"]\n",
    "                }\n",
    "            }],\n",
    "            input=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{ \"type\": \"input_text\", \"text\": \"Can you make a simple word document for lesson plan on the topic of 'The Importance of Water' for a 5 year old child? Please ensure you output a word document. You must include the file you generate in the annotation of the output text using a markdown url link in this format as an example: sandbox:/mnt/data/int100.txt\" }]\n",
    "            }],\n",
    "            reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "            text={\"format\": {\"type\": \"text\"}},\n",
    "            include=[\"code_interpreter_call.outputs\"],\n",
    "            max_output_tokens=32000\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_code_interpreter()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"type\":\"response.output_item.done\",\"sequence_number\":877,\"output_index\":2,\"item\":{\"id\":\"msg_6838c205884c8191a9ef3e5e8c5378170a1145d46963df26\",\"type\":\"message\",\"status\":\"completed\",\"content\":[{\"type\":\"output_text\",\"annotations\":[{\"type\":\"container_file_citation\",\"container_id\":\"cntr_6838c1f76ff88191b27d1dcd064592550c88636f8949b334\",\"end_index\":219,\"file_id\":\"cfile_6838c20627bc81919fbf3bae04fa20e2\",\"filename\":\"lesson_plan_importance_of_water.docx\",\"start_index\":165}],\"text\":\"I’ve created the lesson plan document for \\\"The Importance of Water\\\" suitable for a 5-year-old. You can download it here:\\n\\n[Download the lesson plan (Word document)](sandbox:/mnt/data/lesson_plan_importance_of_water.docx)\"}],\"role\":\"assistant\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "async def run_stream():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"what do these images show? \",\n",
    "            \"images\":[\n",
    "                \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/ca106aaf-bfa3-45b9-bf70-eb612bbe27d0.png\",\n",
    "                \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/00b5fe2d-ff07-43e1-8f2e-ab73906a30b7.png\",\n",
    "                \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/2cc690db-8cc4-4737-8e97-ac96b4256793.png\"\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response_generator = await llm.query(\n",
    "            # model_name=\"o3-mini\",\n",
    "            reasoning_effort=\"none\",\n",
    "            model_name=\"gemini-2.5-flash-preview-04-17\",\n",
    "            # model_name=\"sonar-pro\",\n",
    "            # model_name=\"responses-o4-mini\",\n",
    "            # reasoning={\"effort\": \"high\", \"summary\": \"auto\"},\n",
    "            # max_output_tokens=4096,\n",
    "            # max_tokens=4096,\n",
    "            # model_name=\"accounts/fireworks/models/deepseek-r1\",\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            # fallback_provider=\"openai\",\n",
    "            # fallback_model=\"gpt-4o\",\n",
    "            moderation=False,\n",
    "            # extra_body={\n",
    "            #     \"return_images\": True,\n",
    "            #     \"web_search_options\": {\n",
    "            #         \"search_context_size\": \"low\"\n",
    "            #     }\n",
    "            # },\n",
    "        )\n",
    "        \n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# This line is key in Jupyter\n",
    "await run_stream()  # Don't use asyncio.run() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            # model_name=\"accounts/fireworks/models/deepseek-r1\",\n",
    "            model_name=\"gemini-2.0-flash\",\n",
    "            # reasoning_effort=\"low\",\n",
    "            messages=messages,\n",
    "            stream=False,  # Set to False for non-streaming\n",
    "            # temperature=0.5,\n",
    "            fallback_provider=\"openai\",\n",
    "            fallback_model=\"gpt-4o\",\n",
    "            moderation=False\n",
    "        )\n",
    "        \n",
    "        # Print the full response content\n",
    "        print(response.content)\n",
    "        \n",
    "        # You can also access other metadata\n",
    "        print(\"\\n--- Response Metadata ---\")\n",
    "        print(f\"Model: {response.model_name}\")\n",
    "        print(f\"Input tokens: {response.usage.input_tokens}\")\n",
    "        print(f\"Output tokens: {response.usage.output_tokens}\")\n",
    "        print(f\"Cost: ${response.cost:.6f}\")\n",
    "        print(f\"Latency: {response.latency:.2f} seconds\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def test_perplexity():\n",
    "    # Sample messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an artificial intelligence assistant and you need to engage in a helpful, detailed, polite conversation with a user. Answer as concisely as possible.\"\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many stars are in the universe?\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Test non-streaming query\n",
    "    print(\"Testing non-streaming Perplexity API with citations\")\n",
    "    response = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Response content: {response.content}\")\n",
    "    # print(f\"Citations: {response.citations}\")\n",
    "    \n",
    "    # Test streaming query\n",
    "    print(\"\\nTesting streaming Perplexity API with citations\")\n",
    "    stream_generator = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    citations_found = False\n",
    "    \n",
    "    async for chunk in stream_generator:\n",
    "        if chunk.startswith(\"\\n<citations>\"):\n",
    "            print(f\"Found citations in stream: {chunk}\")\n",
    "            citations_found = True\n",
    "        else:\n",
    "            full_response += chunk\n",
    "            print(f\"Received chunk: {chunk}\")\n",
    "    \n",
    "    print(f\"\\nFull response: {full_response[:100]}...\")\n",
    "    \n",
    "    # After streaming is complete, check if provider has citations\n",
    "    provider = llm._get_provider(\"sonar\")\n",
    "    if hasattr(provider, 'last_citations') and provider.last_citations:\n",
    "        print(f\"Citations from provider.last_citations: {provider.last_citations}\")\n",
    "        citations_found = True\n",
    "    \n",
    "    if not citations_found:\n",
    "        print(\"No citations found in streaming response\")\n",
    "\n",
    "await test_perplexity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"A cat on its back legs running like a human is holding a big silver fish with its arms. The cat is running away from the shop owner and has a panicked look on his face. The scene is situated in a crowded market.\"\n",
    "\n",
    "try:\n",
    "    # Generate the image\n",
    "    response = await llm.query(\n",
    "        model_name=\"recraftv3\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        style=\"digital_illustration\"\n",
    "    )\n",
    "    \n",
    "    # Print the result (which is the image URL)\n",
    "    print(f\"Image generation successful! URL: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Narrate the text: \"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o3\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"search_context_size\": \"low\",\n",
    "    }],\n",
    "    input=\"What movie won best picture in 2025?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uflo AI Server",
   "language": "python",
   "name": "ai-server"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
