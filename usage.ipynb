{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import sys\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sina/Desktop/Uflo Platform/uflo-AI-server/ai-server/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:llm_master.response_synthesizer:Initialized QueryLLM handler with rate limiters\n"
     ]
    }
   ],
   "source": [
    "import time, base64, json, requests, asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import List, Dict, Union, Any \n",
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', force=True)     \n",
    "# Import our providers\n",
    "from llm_master import QueryLLM, LLMConfig\n",
    "config = LLMConfig.from_env()\n",
    "llm = QueryLLM(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.openai_provider:Successfully initialized OpenAI provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text response:\n",
      "Instantiating provider: openai_provider with class OpenAIProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```py\n",
      "from docx import Document\n",
      "\n",
      "# Create a new Word document\n",
      "doc = Document()\n",
      "\n",
      "# Add title and basic information\n",
      "doc.add_heading(\"Lesson Plan: The Importance of Water\", level=1)\n",
      "doc.add_paragraph(\"Age Group: 5 year olds\")\n",
      "doc.add_paragraph(\"Duration: 30 minutes\")\n",
      "\n",
      "# Add objectives\n",
      "doc.add_heading(\"Objectives\", level=2)\n",
      "objectives = [\n",
      "    \"Understand why water is important for life\",\n",
      "    \"Identify sources of water\",\n",
      "    \"Practice good water use habits\"\n",
      "]\n",
      "for obj in objectives:\n",
      "    doc.add_paragraph(f\"• {obj}\")\n",
      "\n",
      "# Add materials\n",
      "doc.add_heading(\"Materials\", level=2)\n",
      "materials = [\n",
      "    \"A glass of water\",\n",
      "    \"Pictures of water uses (plants, animals, drinking)\",\n",
      "    \"Coloring sheets with water drops\",\n",
      "    \"Crayons\"\n",
      "]\n",
      "for item in materials:\n",
      "    doc.add_paragraph(f\"• {item}\")\n",
      "\n",
      "# Add lesson outline\n",
      "doc.add_heading(\"Lesson Outline\", level=2)\n",
      "\n",
      "# Introduction\n",
      "doc.add_heading(\"1. Introduction (5 min)\", level=3)\n",
      "intro_steps = [\n",
      "    \"Greet the children.\",\n",
      "    \"Show a glass of water; ask what it is.\"\n",
      "]\n",
      "for step in intro_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Story Time\n",
      "doc.add_heading(\"2. Story Time (5 min)\", level=3)\n",
      "story_steps = [\n",
      "    \"Read a short story about a thirsty plant or animal.\"\n",
      "]\n",
      "for step in story_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Discussion\n",
      "doc.add_heading(\"3. Discussion (5 min)\", level=3)\n",
      "discussion_steps = [\n",
      "    \"Ask: Why do we need water?\",\n",
      "    \"Discuss how plants, animals, and people use water.\"\n",
      "]\n",
      "for step in discussion_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Activity\n",
      "doc.add_heading(\"4. Activity (10 min)\", level=3)\n",
      "activity_steps = [\n",
      "    \"Coloring activity: coloring water drops and pictures.\"\n",
      "]\n",
      "for step in activity_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Practical Demo\n",
      "doc.add_heading(\"5. Practical Demo (5 min)\", level=3)\n",
      "demo_steps = [\n",
      "    \"Show a small plant and water it to demonstrate how plants drink water.\"\n",
      "]\n",
      "for step in demo_steps:\n",
      "    doc.add_paragraph(f\"• {step}\")\n",
      "\n",
      "# Conclusion and Assessment\n",
      "doc.add_heading(\"Conclusion and Assessment\", level=2)\n",
      "doc.add_paragraph(\"Recap key points: water helps us grow, water plants, animals, and ourselves.\")\n",
      "doc.add_paragraph(\"Sing a short water song together.\")\n",
      "doc.add_paragraph(\"Ask each child to name one way they use water.\")\n",
      "\n",
      "# Extensions\n",
      "doc.add_heading(\"Extensions\", level=2)\n",
      "extensions = [\n",
      "    \"Teach simple water-saving tips: turn off the tap while brushing teeth.\",\n",
      "    \"Water a class plant daily and observe growth.\"\n",
      "]\n",
      "for ext in extensions:\n",
      "    doc.add_paragraph(f\"• {ext}\")\n",
      "\n",
      "# Save the document\n",
      "file_path = \"/mnt/data/importance_of_water_lesson_plan.docx\"\n",
      "doc.save(file_path)\n",
      "\n",
      "file_path\n",
      "```\n",
      "\n",
      "```plaintext\n",
      "'/mnt/data/importance_of_water_lesson_plan.docx'\n",
      "```\n",
      "\n",
      "Here is the lesson plan on \"The Importance of Water\" for 5-year-old children. You can download the Word document using the link below:\n",
      "\n",
      "[Download Lesson Plan (Importance_of_Water)](sandbox:/mnt/data/importance_of_water_lesson_plan.docx)<code_execution_file><code_execution_file_id>cfile_6838bbecaa608191a7298200cf3c8eaf</code_execution_file_id><code_execution_file_name>importance_of_water_lesson_plan.docx</code_execution_file_name></code_execution_file>\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "async def run_code_interpreter():\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response_generator = await llm.query(\n",
    "            model_name=\"responses-o4-mini\",\n",
    "            messages=[],\n",
    "            stream=True,\n",
    "            tools=[{\n",
    "                \"type\": \"code_interpreter\",\n",
    "                \"container\": {\n",
    "                    \"type\": \"auto\",\n",
    "                    # \"file_ids\": [\"file-TXT3RH5yycr7MAX2H8kLvq\"]\n",
    "                }\n",
    "            }],\n",
    "            input=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{ \"type\": \"input_text\", \"text\": \"Can you make a simple word document for lesson plan on the topic of 'The Importance of Water' for a 5 year old child? Please ensure you output a word document. You must include the file you generate in the annotation of the output text using a markdown url link in this format as an example: sandbox:/mnt/data/int100.txt\" }]\n",
    "            }],\n",
    "            reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "            text={\"format\": {\"type\": \"text\"}},\n",
    "            include=[\"code_interpreter_call.outputs\"],\n",
    "            max_output_tokens=32000\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_code_interpreter()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"type\":\"response.output_item.done\",\"sequence_number\":877,\"output_index\":2,\"item\":{\"id\":\"msg_6838c205884c8191a9ef3e5e8c5378170a1145d46963df26\",\"type\":\"message\",\"status\":\"completed\",\"content\":[{\"type\":\"output_text\",\"annotations\":[{\"type\":\"container_file_citation\",\"container_id\":\"cntr_6838c1f76ff88191b27d1dcd064592550c88636f8949b334\",\"end_index\":219,\"file_id\":\"cfile_6838c20627bc81919fbf3bae04fa20e2\",\"filename\":\"lesson_plan_importance_of_water.docx\",\"start_index\":165}],\"text\":\"I’ve created the lesson plan document for \\\"The Importance of Water\\\" suitable for a 5-year-old. You can download it here:\\n\\n[Download the lesson plan (Word document)](sandbox:/mnt/data/lesson_plan_importance_of_water.docx)\"}],\"role\":\"assistant\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_stream():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"what do these images show? \",\n",
    "            \"images\":[\n",
    "                \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/ca106aaf-bfa3-45b9-bf70-eb612bbe27d0.png\",\n",
    "                \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/00b5fe2d-ff07-43e1-8f2e-ab73906a30b7.png\",\n",
    "                \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/2cc690db-8cc4-4737-8e97-ac96b4256793.png\"\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response_generator = await llm.query(\n",
    "            # model_name=\"o3-mini\",\n",
    "            reasoning_effort=\"none\",\n",
    "            model_name=\"gemini-2.5-flash-preview-04-17\",\n",
    "            # model_name=\"sonar-pro\",\n",
    "            # model_name=\"responses-o4-mini\",\n",
    "            # reasoning={\"effort\": \"high\", \"summary\": \"auto\"},\n",
    "            # max_output_tokens=4096,\n",
    "            # max_tokens=4096,\n",
    "            # model_name=\"accounts/fireworks/models/deepseek-r1\",\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            # fallback_provider=\"openai\",\n",
    "            # fallback_model=\"gpt-4o\",\n",
    "            moderation=False,\n",
    "            # extra_body={\n",
    "            #     \"return_images\": True,\n",
    "            #     \"web_search_options\": {\n",
    "            #         \"search_context_size\": \"low\"\n",
    "            #     }\n",
    "            # },\n",
    "        )\n",
    "        \n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# This line is key in Jupyter\n",
    "await run_stream()  # Don't use asyncio.run() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.base_provider:Initialized openai provider with base URL: None\n",
      "INFO:llm_master.base_provider:Successfully initialized openai provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating provider: openai with class UnifiedProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s a simple explanation of **quantum computing**:\n",
      "\n",
      "1. **Ordinary computers** (like your laptop or phone) use little units of information called **bits**, which can be either 0 or 1.\n",
      "2. **Quantum computers** use special units called **qubits** (short for quantum bits). Qubits can be 0, 1, or any combination of both at the same time, thanks to something called **superposition**.\n",
      "3. Because of this, quantum computers can process lots of possibilities **at once**, while ordinary computers have to check each possibility one after the other.\n",
      "4. They also use another property called **entanglement**, which means qubits can be linked together so that the state of one qubit depends on another, allowing for even more powerful calculations.\n",
      "5. **In short:** Quantum computers can solve certain problems much faster than ordinary computers, especially really hard problems like breaking codes, searching huge databases, or simulating molecules for new medicines.\n",
      "\n",
      "Think of a quantum computer as a super-powered problem solver that uses the weird rules of quantum physics to do many things at once!\n",
      "\n",
      "--- Response Metadata ---\n",
      "Model: gpt-4.1\n",
      "Input tokens: 13\n",
      "Output tokens: 231\n",
      "Cost: $0.001874\n",
      "Latency: 4.71 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            # model_name=\"accounts/fireworks/models/deepseek-r1\",\n",
    "            model_name=\"gpt-4.1\",\n",
    "            # reasoning_effort=\"low\",\n",
    "            messages=messages,\n",
    "            stream=False,  # Set to False for non-streaming\n",
    "            # temperature=0.5,\n",
    "            fallback_provider=\"openai\",\n",
    "            fallback_model=\"gpt-4o\",\n",
    "            moderation=False\n",
    "        )\n",
    "        \n",
    "        # Print the full response content\n",
    "        print(response.content)\n",
    "        \n",
    "        # You can also access other metadata\n",
    "        print(\"\\n--- Response Metadata ---\")\n",
    "        print(f\"Model: {response.model_name}\")\n",
    "        print(f\"Input tokens: {response.usage.input_tokens}\")\n",
    "        print(f\"Output tokens: {response.usage.output_tokens}\")\n",
    "        print(f\"Cost: ${response.cost:.6f}\")\n",
    "        print(f\"Latency: {response.latency:.2f} seconds\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def test_perplexity():\n",
    "    # Sample messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an artificial intelligence assistant and you need to engage in a helpful, detailed, polite conversation with a user. Answer as concisely as possible.\"\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many stars are in the universe?\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Test non-streaming query\n",
    "    print(\"Testing non-streaming Perplexity API with citations\")\n",
    "    response = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Response content: {response.content}\")\n",
    "    # print(f\"Citations: {response.citations}\")\n",
    "    \n",
    "    # Test streaming query\n",
    "    print(\"\\nTesting streaming Perplexity API with citations\")\n",
    "    stream_generator = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    citations_found = False\n",
    "    \n",
    "    async for chunk in stream_generator:\n",
    "        if chunk.startswith(\"\\n<citations>\"):\n",
    "            print(f\"Found citations in stream: {chunk}\")\n",
    "            citations_found = True\n",
    "        else:\n",
    "            full_response += chunk\n",
    "            print(f\"Received chunk: {chunk}\")\n",
    "    \n",
    "    print(f\"\\nFull response: {full_response[:100]}...\")\n",
    "    \n",
    "    # After streaming is complete, check if provider has citations\n",
    "    provider = llm._get_provider(\"sonar\")\n",
    "    if hasattr(provider, 'last_citations') and provider.last_citations:\n",
    "        print(f\"Citations from provider.last_citations: {provider.last_citations}\")\n",
    "        citations_found = True\n",
    "    \n",
    "    if not citations_found:\n",
    "        print(\"No citations found in streaming response\")\n",
    "\n",
    "await test_perplexity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"A cat on its back legs running like a human is holding a big silver fish with its arms. The cat is running away from the shop owner and has a panicked look on his face. The scene is situated in a crowded market.\"\n",
    "\n",
    "try:\n",
    "    # Generate the image\n",
    "    response = await llm.query(\n",
    "        model_name=\"recraftv3\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        style=\"digital_illustration\"\n",
    "    )\n",
    "    \n",
    "    # Print the result (which is the image URL)\n",
    "    print(f\"Image generation successful! URL: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Narrate the text: \"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gemini caching with long prompt...\n",
      "Prompt length: 15177 characters\n",
      "============================================================\n",
      "\n",
      "🔥 FIRST CALL (no cache):\n",
      "DEBUG: Full Gemini response usage: CompletionUsage(completion_tokens=77, prompt_tokens=3771, total_tokens=4178, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "DEBUG: Gemini response usage dict: {'completion_tokens': 77, 'prompt_tokens': 3771, 'total_tokens': 4178, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "DEBUG: Non-streaming prompt_tokens_details: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.classes:Added 1711 tokens to gemini-2.5-flash. Total usage: 1292636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: This text is the complete Desmos API documentation, designed to teach users how to programmatically create and manipulate graphs. It details the `setExpression` function, explaining how to define and customize mathematical expressions and tables with properties like styling, visibility, and interactive sliders. The document also covers Desmos's LaTeX-based expression syntax and a wide range of supported mathematical and statistical functions.\n",
      "Usage: Usage(input_tokens=3771, output_tokens=77, cached_tokens=0)\n",
      "Cost: $0.001324\n",
      "\n",
      "♻️  SECOND CALL (with previous response + new question):\n",
      "DEBUG: Full Gemini response usage: CompletionUsage(completion_tokens=239, prompt_tokens=3878, total_tokens=5261, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "DEBUG: Gemini response usage dict: {'completion_tokens': 239, 'prompt_tokens': 3878, 'total_tokens': 5261, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "DEBUG: Non-streaming prompt_tokens_details: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.classes:Added 1711 tokens to gemini-2.5-flash. Total usage: 1294347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the documentation and summary, here are the three most important features of the Desmos API for a beginner developer:\n",
      "\n",
      "1.  **Adding and Customizing Expressions (`calculator.setExpression` with `latex` and Styling):** The fundamental ability to put mathematical content on the graph using the `latex` property, along with immediate visual customization options like `color`, `lineStyle`, `pointStyle`, `lineWidth`, and `pointSize`, is crucial for any beginner to see their graphs come to life.\n",
      "2.  **Creating Interactive Sliders (`slider` property):** Desmos is renowned for its interactive capabilities. The `slider` object, which allows defining variables with `min`, `max`, `step`, and animation `loopMode`, is an incredibly powerful and accessible way for beginners to add dynamic behavior and exploration to their graphs.\n",
      "3.  **Controlling Point Behavior (`dragMode`):** For expressions involving points, the `dragMode` property (e.g., `Desmos.DragModes.XY`, `NONE`) allows beginners to easily define how users can interact with points directly on the graph, enabling more intuitive and manipulative explorations.\n",
      "Usage: Usage(input_tokens=3878, output_tokens=239, cached_tokens=0)\n",
      "Cost: $0.001761\n",
      "\n",
      "🌊 STREAMING CALL (with conversation context):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Full Gemini chunk: ChatCompletionChunk(id='yilzaIKqMoS6qtsPx-OA4QI', choices=[Choice(delta=ChoiceDelta(content='In my opinion, the most confusing part of the Desmos API documentation for a beginner developer is the interaction between JavaScript string escaping and LaTeX syntax, specifically when dealing with', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1752377808, model='gemini-2.5-flash', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=33, prompt_tokens=4134, total_tokens=5447, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "DEBUG: Gemini usage object: CompletionUsage(completion_tokens=33, prompt_tokens=4134, total_tokens=5447, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "DEBUG: Gemini usage dict: {'completion_tokens': 33, 'prompt_tokens': 4134, 'total_tokens': 5447, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "DEBUG: prompt_tokens_details: None\n",
      "DEBUG: Full Gemini chunk: ChatCompletionChunk(id='yilzaIKqMoS6qtsPx-OA4QI', choices=[Choice(delta=ChoiceDelta(content=' **backslashes (`\\\\`) for mathematical functions and symbols**.\\n\\nThe documentation explicitly highlights this: \"That the backslash also functions as an escape character inside of JavaScript strings is a common source of confusion.\" For a beginner, having to remember to double-escape', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1752377809, model='gemini-2.5-flash', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=86, prompt_tokens=4134, total_tokens=5500, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "DEBUG: Gemini usage object: CompletionUsage(completion_tokens=86, prompt_tokens=4134, total_tokens=5500, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "DEBUG: Gemini usage dict: {'completion_tokens': 86, 'prompt_tokens': 4134, 'total_tokens': 5500, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "DEBUG: prompt_tokens_details: None\n",
      "DEBUG: Full Gemini chunk: ChatCompletionChunk(id='yilzaIKqMoS6qtsPx-OA4QI', choices=[Choice(delta=ChoiceDelta(content=\" every backslash (`\\\\\\\\sin(\\\\\\\\pi)`) or use the less common `String.raw` template literals to correctly pass LaTeX strings can be a significant hurdle. It's not a conceptual difficulty with Desmos's features, but rather a technical\", function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1752377809, model='gemini-2.5-flash', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=137, prompt_tokens=4134, total_tokens=5551, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "DEBUG: Gemini usage object: CompletionUsage(completion_tokens=137, prompt_tokens=4134, total_tokens=5551, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "DEBUG: Gemini usage dict: {'completion_tokens': 137, 'prompt_tokens': 4134, 'total_tokens': 5551, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "DEBUG: prompt_tokens_details: None\n",
      "DEBUG: Full Gemini chunk: ChatCompletionChunk(id='yilzaIKqMoS6qtsPx-OA4QI', choices=[Choice(delta=ChoiceDelta(content=\" detail related to JavaScript's string handling that can lead to frustrating syntax errors before any actual graphing even happens.\", function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1752377809, model='gemini-2.5-flash', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=159, prompt_tokens=4134, total_tokens=5573, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "DEBUG: Gemini usage object: CompletionUsage(completion_tokens=159, prompt_tokens=4134, total_tokens=5573, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "DEBUG: Gemini usage dict: {'completion_tokens': 159, 'prompt_tokens': 4134, 'total_tokens': 5573, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
      "DEBUG: prompt_tokens_details: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.classes:Added 4293 tokens to gemini-2.5-flash. Total usage: 1298640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamed response: In my opinion, the most confusing part of the Desmos API documentation for a beginner developer is the interaction between JavaScript string escaping and LaTeX syntax, specifically when dealing with **backslashes (`\\`) for mathematical functions and symbols**.\n",
      "\n",
      "The documentation explicitly highlights this: \"That the backslash also functions as an escape character inside of JavaScript strings is a common source of confusion.\" For a beginner, having to remember to double-escape every backslash (`\\\\sin(\\\\pi)`) or use the less common `String.raw` template literals to correctly pass LaTeX strings can be a significant hurdle. It's not a conceptual difficulty with Desmos's features, but rather a technical detail related to JavaScript's string handling that can lead to frustrating syntax errors before any actual graphing even happens.\n",
      "Usage: Usage(input_tokens=4134, output_tokens=159, cached_tokens=0)\n",
      "Cost: $0.001638\n",
      "\n",
      "============================================================\n",
      "🔍 ANALYSIS:\n",
      "Call 1 tokens: 3771 input, 77 output, 0 cached\n",
      "Call 2 tokens: 3878 input, 239 output, 0 cached\n",
      "Stream tokens: 4134 input, 159 output, 0 cached\n",
      "❌ No cache hits detected\n"
     ]
    }
   ],
   "source": [
    "async def test_gemini_caching_with_long_prompt():\n",
    "    \"\"\"Test Gemini caching with a very long prompt from desmos documentation\"\"\"\n",
    "    \n",
    "    # Read the long Desmos documentation\n",
    "    with open(\"../chat/prompts/desmos_prompt.txt\", \"r\") as f:\n",
    "        desmos_docs = f.read()\n",
    "    desmos_docs = desmos_docs[:15000]\n",
    "    # Create a long prompt with the documentation\n",
    "    long_prompt = f\"\"\"Here is the complete Desmos API documentation:\n",
    "\n",
    "{desmos_docs}\n",
    "\n",
    "Based on this documentation, please answer this simple question: What is this text about? Give me a brief 2-3 sentence summary.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": long_prompt}]\n",
    "    \n",
    "    print(\"Testing Gemini caching with long prompt...\")\n",
    "    print(f\"Prompt length: {len(long_prompt)} characters\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test 1: First call (should not have cache hits)\n",
    "    print(\"\\n🔥 FIRST CALL (no cache):\")\n",
    "    response1 = await llm.query(\n",
    "        model_name=\"gemini-2.5-flash\",\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"Response: {response1.content}\")\n",
    "    print(f\"Usage: {response1.usage}\")\n",
    "    print(f\"Cost: ${response1.cost:.6f}\")\n",
    "    time.sleep(10)\n",
    "    # Test 2: Second call with previous response + new question (should have cache hits if caching works)\n",
    "    print(\"\\n♻️  SECOND CALL (with previous response + new question):\")\n",
    "    \n",
    "    # Create new messages including the previous response and a new question\n",
    "    messages_with_response = [\n",
    "        {\"role\": \"user\", \"content\": long_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response1.content},\n",
    "        {\"role\": \"user\", \"content\": \"Based on your summary, can you now tell me what are the 3 most important features of the Desmos API for a beginner developer?\"}\n",
    "    ]\n",
    "    \n",
    "    response2 = await llm.query(\n",
    "        model_name=\"gemini-2.5-flash\", \n",
    "        messages=messages_with_response,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"Response: {response2.content}\")\n",
    "    print(f\"Usage: {response2.usage}\")\n",
    "    print(f\"Cost: ${response2.cost:.6f}\")\n",
    "    \n",
    "    # Test 3: Streaming version with same conversation context\n",
    "    print(\"\\n🌊 STREAMING CALL (with conversation context):\")\n",
    "    messages_with_response2 = [\n",
    "        {\"role\": \"user\", \"content\": long_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response1.content},\n",
    "        {\"role\": \"user\", \"content\": \"Based on your summary, can you now tell me what are the 3 most important features of the Desmos API for a beginner developer?\"},\n",
    "        {\"role\": \"assistant\", \"content\": response2.content},\n",
    "        {\"role\": \"user\", \"content\": \"What is the most confusing part of the Desmos API in your opinion?\"}\n",
    "    ]\n",
    "    stream = await llm.query(\n",
    "        model_name=\"gemini-2.5-flash\",\n",
    "        messages=messages_with_response2, \n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_content = \"\"\n",
    "    async for chunk in stream:\n",
    "        full_content += chunk\n",
    "    \n",
    "    provider = llm._get_provider(\"gemini-2.5-flash\")\n",
    "    if hasattr(provider, 'last_usage') and provider.last_usage:\n",
    "        print(f\"Streamed response: {full_content}\")\n",
    "        print(f\"Usage: {provider.last_usage}\")\n",
    "        try:\n",
    "            from llm_master.classes import ModelRegistry\n",
    "            model_config = ModelRegistry.get_config(\"gemini-2.5-flash\")\n",
    "            cost = provider.last_usage.calculate_cost(model_config)\n",
    "            print(f\"Cost: ${cost:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cost calculation failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🔍 ANALYSIS:\")\n",
    "    print(f\"Call 1 tokens: {response1.usage.input_tokens} input, {response1.usage.output_tokens} output, {response1.usage.cached_tokens} cached\")\n",
    "    print(f\"Call 2 tokens: {response2.usage.input_tokens} input, {response2.usage.output_tokens} output, {response2.usage.cached_tokens} cached\")\n",
    "    if hasattr(provider, 'last_usage') and provider.last_usage:\n",
    "        print(f\"Stream tokens: {provider.last_usage.input_tokens} input, {provider.last_usage.output_tokens} output, {provider.last_usage.cached_tokens} cached\")\n",
    "    \n",
    "    if response2.usage.cached_tokens > 0:\n",
    "        print(\"🎉 CACHING IS WORKING!\")\n",
    "    else:\n",
    "        print(\"❌ No cache hits detected\")\n",
    "\n",
    "# Run the test\n",
    "await test_gemini_caching_with_long_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o3\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"search_context_size\": \"low\",\n",
    "    }],\n",
    "    input=\"What movie won best picture in 2025?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uflo AI Server",
   "language": "python",
   "name": "ai-server"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
