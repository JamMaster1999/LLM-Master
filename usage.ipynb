{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import sys\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sina/Desktop/Uflo Platform/uflo-AI-server/ai-server/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:llm_master.response_synthesizer:Initialized QueryLLM handler with rate limiters\n"
     ]
    }
   ],
   "source": [
    "import time, base64, json, requests, asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import List, Dict, Union, Any \n",
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', force=True)     \n",
    "# Import our providers\n",
    "from llm_master import QueryLLM, LLMConfig\n",
    "config = LLMConfig.from_env()\n",
    "llm = QueryLLM(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.openai_provider:Successfully initialized OpenAI provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text response:\n",
      "Instantiating provider: openai_provider with class OpenAIProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```py\n",
      "import docx\n",
      "doc = docx.Document()\n",
      "doc.add_paragraph(\"Test\")\n",
      "doc.save(\"/mnt/data/test.docx\")\n",
      "\"/mnt/data/test.docx exists\"\n",
      "```\n",
      "\n",
      "\n",
      "```plaintext\n",
      "'/mnt/data/test.docx exists'\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "<think>\n",
      "\n",
      "**Generating docx file**\n",
      "\n",
      "I'm excited to generate a docx file for the user! I need to run code to create the final document, but since the output won’t be visible to the user until they see the link, that's okay. I’ll prepare the lesson plan, which will include the title, age group, objectives, materials, introduction, and main activity. Once the file is created, I’ll provide the link using markdown for easy access. Let's get started on writing the code!**Creating lesson plan document**\n",
      "\n",
      "I’m focusing on preparing a lesson plan about the importance of water. I want to include simple facts like how water keeps us alive and helps plants grow. I'll add some discussion questions, a recap for the conclusion, and an extension activity involving drawing water habits and discussing conservation. Once I've written the necessary paragraphs and headings, I'll save the document. Let’s get to work on generating the docx file for the user!\n",
      "```py\n",
      "from docx import Document\n",
      "\n",
      "doc = Document()\n",
      "doc.add_heading('Lesson Plan: The Importance of Water', level=1)\n",
      "\n",
      "# Basic info\n",
      "doc.add_paragraph('Age Group: 5-year-old children')\n",
      "doc.add_paragraph('Duration: 30 minutes')\n",
      "doc.add_paragraph('Objective:')\n",
      "doc.add_paragraph('  • Understand what water is', style='List Bullet')\n",
      "doc.add_paragraph('  • Recognize why water is important for life', style='List Bullet')\n",
      "doc.add_paragraph('  • Identify ways to use and conserve water', style='List Bullet')\n",
      "\n",
      "# Materials\n",
      "doc.add_heading('Materials', level=2)\n",
      "doc.add_paragraph('- A clear glass of water')\n",
      "doc.add_paragraph('- Small bowls or containers')\n",
      "doc.add_paragraph('- Coloring (food coloring or washable paint)')\n",
      "doc.add_paragraph('- Picture book or images of water in nature')\n",
      "doc.add_paragraph('- Cups and spoons for pouring activity')\n",
      "\n",
      "# Introduction\n",
      "doc.add_heading('Introduction (5 minutes)', level=2)\n",
      "doc.add_paragraph('1. Greet the child and show them a glass of water.')\n",
      "doc.add_paragraph('2. Ask: \"What do you think this is?\" and listen to their answers.')\n",
      "doc.add_paragraph('3. Explain: \"This is water. Everyone and everything needs water to live.\"')\n",
      "\n",
      "# Main Activity\n",
      "doc.add_heading('Main Activity (15 minutes)', level=2)\n",
      "doc.add_paragraph('Activity 1: Exploring Water Sensory Play')\n",
      "doc.add_paragraph('  • Let the child pour water between bowls with cups and spoons.')\n",
      "doc.add_paragraph('  • Ask them how the water feels. Is it cold? Smooth?')\n",
      "doc.add_paragraph('Activity 2: Coloring Water')\n",
      "doc.add_paragraph('  • Add a drop of food coloring to water and watch it change color.')\n",
      "doc.add_paragraph('  • Talk about how water can mix with things to change.')\n",
      "\n",
      "# Discussion\n",
      "doc.add_heading('Discussion (5 minutes)', level=2)\n",
      "doc.add_paragraph('Ask questions:')\n",
      "doc.add_paragraph('  • Why do you think we need to drink water?')\n",
      "doc.add_paragraph('  • What happens if we do not have water to drink?')\n",
      "doc.add_paragraph('  • Can you think of other things that use water? (Plants, animals)')\n",
      "\n",
      "# Conclusion\n",
      "doc.add_heading('Conclusion (5 minutes)', level=2)\n",
      "doc.add_paragraph('1. Summarize key points: water helps us stay healthy, helps plants grow, and we can play with it.')\n",
      "doc.add_paragraph('2. Encourage the child to drink a glass of water now.')\n",
      "doc.add_paragraph('3. Praise their observations and answers.')\n",
      "\n",
      "# Extension Activity (Optional)\n",
      "doc.add_heading('Extension Activities', level=2)\n",
      "doc.add_paragraph('- Draw a picture of something that needs water (e.g., a plant, fish, you drinking).')\n",
      "doc.add_paragraph('- Go on a \"water walk\": look for water at home or outside (sink, garden).')\n",
      "\n",
      "# Save document\n",
      "file_path = '/mnt/data/lesson_plan_the_importance_of_water.docx'\n",
      "doc.save(file_path)\n",
      "file_path\n",
      "```\n",
      "\n",
      "\n",
      "```plaintext\n",
      "'/mnt/data/lesson_plan_the_importance_of_water.docx'\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "</think>\n",
      "\n",
      "I’ve created the lesson plan as a Word document. You can download it here:\n",
      "\n",
      "[Download the lesson plan: The Importance of Water (5-year-olds)](sandbox:/mnt/data/lesson_plan_the_importance_of_water.docx)\n",
      "\n",
      "<code_execution_file><code_execution_file_id>cfile_68e9be5e56b88191a271c96145841d93</code_execution_file_id><code_execution_file_name>lesson_plan_the_importance_of_water.docx</code_execution_file_name><code_execution_container_id>cntr_68e9be4af6a081918db5de5fddc0c87a00b9f9fec945e8db</code_execution_container_id></code_execution_file>\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "async def run_code_interpreter():\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response_generator = await llm.query(\n",
    "            model_name=\"responses-o4-mini\",\n",
    "            messages=[],\n",
    "            stream=True,\n",
    "            tools=[{\n",
    "                \"type\": \"code_interpreter\",\n",
    "                \"container\": {\n",
    "                    \"type\": \"auto\",\n",
    "                    # \"file_ids\": [\"file-TXT3RH5yycr7MAX2H8kLvq\"]\n",
    "                }\n",
    "            }],\n",
    "            input=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{ \"type\": \"input_text\", \"text\": \"Can you make a simple word document for lesson plan on the topic of 'The Importance of Water' for a 5 year old child? Please ensure you output a word document. You must include the file you generate in the annotation of the output text using a markdown url link in this format as an example: sandbox:/mnt/data/int100.txt\" }]\n",
    "            }],\n",
    "            reasoning={\"effort\": \"medium\", \"summary\": \"auto\"},\n",
    "            text={\"format\": {\"type\": \"text\"}},\n",
    "            include=[\"code_interpreter_call.outputs\"],\n",
    "            max_output_tokens=32000\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_code_interpreter()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"type\":\"response.output_item.done\",\"sequence_number\":877,\"output_index\":2,\"item\":{\"id\":\"msg_6838c205884c8191a9ef3e5e8c5378170a1145d46963df26\",\"type\":\"message\",\"status\":\"completed\",\"content\":[{\"type\":\"output_text\",\"annotations\":[{\"type\":\"container_file_citation\",\"container_id\":\"cntr_6838c1f76ff88191b27d1dcd064592550c88636f8949b334\",\"end_index\":219,\"file_id\":\"cfile_6838c20627bc81919fbf3bae04fa20e2\",\"filename\":\"lesson_plan_importance_of_water.docx\",\"start_index\":165}],\"text\":\"I’ve created the lesson plan document for \\\"The Importance of Water\\\" suitable for a 5-year-old. You can download it here:\\n\\n[Download the lesson plan (Word document)](sandbox:/mnt/data/lesson_plan_importance_of_water.docx)\"}],\"role\":\"assistant\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These three images show different visualizations of Latin American demographic and economic data for 2025:\n",
      "\n",
      "## Image 1: Population and GDP Growth (Combination Chart)\n",
      "This chart displays:\n",
      "- **Blue bars**: Population sizes for 9 Latin American countries\n",
      "- **Green line with dots**: GDP growth rates (%)\n",
      "- Shows Brazil and Mexico have the largest populations (around 200M+ and 125M respectively)\n",
      "- Venezuela shows the highest GDP growth rate (~4.0%)\n",
      "- Ecuador has the lowest GDP growth rate (~1.9%)\n",
      "- There's no direct correlation between population size and GDP growth\n",
      "\n",
      "## Image 2: Population Distribution (Pie Chart)\n",
      "This shows the relative share of population among 10 Latin American countries:\n",
      "- **Brazil dominates** with 37.3% of the total population\n",
      "- **Mexico** is second with 23.2%\n",
      "- Together, Brazil and Mexico account for over 60% of the region's population\n",
      "- Smaller countries like Cuba (2.1%), Guatemala (2.9%), and Chile (3.3%) have much smaller shares\n",
      "\n",
      "## Image 3: Population by Country (Bar Chart)\n",
      "A simple bar chart showing absolute population values:\n",
      "- Arranged in descending order from Brazil (~205M) to Cuba (~11M)\n",
      "- Provides a clear visual comparison of population sizes\n",
      "- Same countries as the first chart, showing the population hierarchy\n",
      "\n",
      "Together, these visualizations provide a comprehensive view of Latin America's demographic landscape and economic growth patterns in 2025."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.classes:Added 5023 tokens to claude-sonnet-4-5-20250929. Total usage: 10542\n"
     ]
    }
   ],
   "source": [
    "async def run_stream():\n",
    "    # messages = [\n",
    "    #     {\n",
    "    #         \"role\": \"user\", \n",
    "    #         \"content\": \"what do these images show? \",\n",
    "    #         \"images\":[\n",
    "    #             \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/ca106aaf-bfa3-45b9-bf70-eb612bbe27d0.png\",\n",
    "    #             \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/00b5fe2d-ff07-43e1-8f2e-ab73906a30b7.png\",\n",
    "    #             \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/2cc690db-8cc4-4737-8e97-ac96b4256793.png\"\n",
    "    #         ]\n",
    "    #     },\n",
    "    # ]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [\n",
    "                \"You are a visual narrative analyst helping me study a storyboard sequence.\",\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Step 1 – describe the setting and dominant mood in the first frame before you see anything else.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"url\": \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/ca106aaf-bfa3-45b9-bf70-eb612bbe27d0.png\",\n",
    "                    \"detail\": \"high\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Step 2 – compare that first frame with the second image, focusing specifically on the change in camera angle and the subject’s posture.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"url\": \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/00b5fe2d-ff07-43e1-8f2e-ab73906a30b7.png\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Step 3 – after reflecting on that comparison, use it to predict the emotional beat captured in the final image.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"url\": \"https://uflo-chat-attachments.s3.us-west-1.amazonaws.com/218/215c88fa-952f-4a21-80d7-2af01ef1c191/2cc690db-8cc4-4737-8e97-ac96b4256793.png\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Deliver the answer with three sections titled Step 1, Step 2, and Step 3 so I can verify you tracked the interleaved instructions correctly.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response_generator = await llm.query(\n",
    "            # reasoning_effort=\"none\",\n",
    "            model_name=\"claude-sonnet-4-5-20250929\",\n",
    "            # max_output_tokens=4096,\n",
    "            # max_tokens=4096,\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "            moderation=False,\n",
    "\n",
    "        )\n",
    "        \n",
    "        async for chunk in response_generator:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# This line is key in Jupyter\n",
    "await run_stream()  # Don't use asyncio.run() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.base_provider:Initialized openai provider with base URL: None\n",
      "INFO:llm_master.base_provider:Successfully initialized openai provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating provider: openai with class UnifiedProvider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s a simple explanation of **quantum computing**:\n",
      "\n",
      "1. **Ordinary computers** (like your laptop or phone) use little units of information called **bits**, which can be either 0 or 1.\n",
      "2. **Quantum computers** use special units called **qubits** (short for quantum bits). Qubits can be 0, 1, or any combination of both at the same time, thanks to something called **superposition**.\n",
      "3. Because of this, quantum computers can process lots of possibilities **at once**, while ordinary computers have to check each possibility one after the other.\n",
      "4. They also use another property called **entanglement**, which means qubits can be linked together so that the state of one qubit depends on another, allowing for even more powerful calculations.\n",
      "5. **In short:** Quantum computers can solve certain problems much faster than ordinary computers, especially really hard problems like breaking codes, searching huge databases, or simulating molecules for new medicines.\n",
      "\n",
      "Think of a quantum computer as a super-powered problem solver that uses the weird rules of quantum physics to do many things at once!\n",
      "\n",
      "--- Response Metadata ---\n",
      "Model: gpt-4.1\n",
      "Input tokens: 13\n",
      "Output tokens: 231\n",
      "Cost: $0.001874\n",
      "Latency: 4.71 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            # model_name=\"accounts/fireworks/models/deepseek-r1\",\n",
    "            model_name=\"gpt-4.1\",\n",
    "            # reasoning_effort=\"low\",\n",
    "            messages=messages,\n",
    "            stream=False,  # Set to False for non-streaming\n",
    "            # temperature=0.5,\n",
    "            fallback_provider=\"openai\",\n",
    "            fallback_model=\"gpt-4o\",\n",
    "            moderation=False\n",
    "        )\n",
    "        \n",
    "        # Print the full response content\n",
    "        print(response.content)\n",
    "        \n",
    "        # You can also access other metadata\n",
    "        print(\"\\n--- Response Metadata ---\")\n",
    "        print(f\"Model: {response.model_name}\")\n",
    "        print(f\"Input tokens: {response.usage.input_tokens}\")\n",
    "        print(f\"Output tokens: {response.usage.output_tokens}\")\n",
    "        print(f\"Cost: ${response.cost:.6f}\")\n",
    "        print(f\"Latency: {response.latency:.2f} seconds\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def test_perplexity():\n",
    "    # Sample messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an artificial intelligence assistant and you need to engage in a helpful, detailed, polite conversation with a user. Answer as concisely as possible.\"\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many stars are in the universe?\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Test non-streaming query\n",
    "    print(\"Testing non-streaming Perplexity API with citations\")\n",
    "    response = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=False,\n",
    "        extra_body={\n",
    "            \"return_images\": True,\n",
    "            \"web_search_options\": {\n",
    "                \"search_context_size\": \"low\"\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    print(f\"Response content: {response.content}\")\n",
    "    # print(f\"Citations: {response.citations}\")\n",
    "    \n",
    "    # Test streaming query\n",
    "    print(\"\\nTesting streaming Perplexity API with citations\")\n",
    "    stream_generator = await llm.query(\n",
    "        model_name=\"sonar\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    citations_found = False\n",
    "    \n",
    "    async for chunk in stream_generator:\n",
    "        if chunk.startswith(\"\\n<citations>\"):\n",
    "            print(f\"Found citations in stream: {chunk}\")\n",
    "            citations_found = True\n",
    "        else:\n",
    "            full_response += chunk\n",
    "            print(f\"Received chunk: {chunk}\")\n",
    "    \n",
    "    print(f\"\\nFull response: {full_response[:100]}...\")\n",
    "    \n",
    "    # After streaming is complete, check if provider has citations\n",
    "    provider = llm._get_provider(\"sonar\")\n",
    "    if hasattr(provider, 'last_citations') and provider.last_citations:\n",
    "        print(f\"Citations from provider.last_citations: {provider.last_citations}\")\n",
    "        citations_found = True\n",
    "    \n",
    "    if not citations_found:\n",
    "        print(\"No citations found in streaming response\")\n",
    "\n",
    "await test_perplexity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"A cat on its back legs running like a human is holding a big silver fish with its arms. The cat is running away from the shop owner and has a panicked look on his face. The scene is situated in a crowded market.\"\n",
    "\n",
    "try:\n",
    "    # Generate the image\n",
    "    response = await llm.query(\n",
    "        model_name=\"recraftv3\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        style=\"digital_illustration\"\n",
    "    )\n",
    "    \n",
    "    # Print the result (which is the image URL)\n",
    "    print(f\"Image generation successful! URL: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Explain quantum computing in simple terms\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def run_query():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Narrate the text: \"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # With stream=False, you get a LLMResponse object directly\n",
    "        response = await llm.query(\n",
    "            model_name=\"gpt-4o-mini-audio-preview\",\n",
    "            messages=messages,\n",
    "            stream=False,\n",
    "            modality=[\"text\", \"audio\"],\n",
    "            audio={\"voice\": \"ash\", \"format\": \"wav\"}\n",
    "        )\n",
    "        \n",
    "         # Print the text response\n",
    "        print(\"Text response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Save the audio to a file if available\n",
    "        if response.audio_data:\n",
    "            wav_bytes = base64.b64decode(response.audio_data)\n",
    "            output_file = \"dog_response.wav\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(wav_bytes)\n",
    "            print(f\"\\nAudio saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(\"\\nNo audio data received in the response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function with await\n",
    "await run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gemini caching with long prompt...\n",
      "Prompt length: 25541 characters\n",
      "============================================================\n",
      "\n",
      "🔥 FIRST CALL (no cache):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.classes:Added 2816 tokens to gemini-2.5-pro. Total usage: 474060375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Of course! It's a great idea to make sure you understand the big picture before diving into the details.\n",
      "\n",
      "Instead of me giving you a summary, let's work it out together. Looking at the main headings and instructions, who do you think this document is written for?\n",
      "Usage: Usage(input_tokens=5093, output_tokens=58, cached_tokens=0)\n",
      "Cost: $0.006946\n",
      "\n",
      "♻️  SECOND CALL (with previous response + new question):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_master.classes:Added 2816 tokens to gemini-2.5-pro. Total usage: 474067181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: That's an excellent question to dig into next. The document definitely has some recurring themes.\n",
      "\n",
      "Based on your reading, if you had to pick one \"golden rule\" that a tutor must absolutely follow, what do you think it would be?\n",
      "Usage: Usage(input_tokens=5178, output_tokens=50, cached_tokens=4072)\n",
      "Cost: $0.003145\n",
      "\n",
      "🌊 STREAMING CALL (with conversation context):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_master.classes:Added 5330 tokens to gemini-2.5-pro. Total usage: 474079229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamed response: That's a very insightful question. It's one thing to read the theory, and another to put it into practice.\n",
      "\n",
      "Thinking about the different roles a tutor has to play according to this document—like asking good questions, figuring out what the student *doesn't* know, and not giving away the answer—which of those tasks do you think would require the most skill or be the easiest to get wrong?\n",
      "Usage: Usage(input_tokens=5245, output_tokens=85, cached_tokens=4066)\n",
      "Cost: $0.003584\n",
      "\n",
      "============================================================\n",
      "🔍 ANALYSIS:\n",
      "Call 1 tokens: 5093 input, 58 output, 0 cached\n",
      "Call 2 tokens: 5178 input, 50 output, 4072 cached\n",
      "Stream tokens: 5245 input, 85 output, 4066 cached\n",
      "🎉 CACHING IS WORKING!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "async def test_gemini_caching_with_long_prompt():\n",
    "    \"\"\"Test Gemini caching with a very long prompt from desmos documentation\"\"\"\n",
    "    with open(\"../chat/prompts/roleplay_prompts.yaml\", \"r\") as f:\n",
    "        roleplay_prompts = yaml.safe_load(f.read())\n",
    "\n",
    "    socratic_prompt = next((item for item in roleplay_prompts if item.get(\"type\") == \"socratic\"), None)\n",
    "\n",
    "    # Create a long prompt with the documentation\n",
    "    long_prompt = f\"\"\"Here is the complete Desmos API documentation:\n",
    "\n",
    "{socratic_prompt.get(\"prompt\")}\n",
    "\n",
    "{socratic_prompt.get(\"prompt\")}\n",
    "\n",
    "Based on this documentation, please answer this simple question: What is this text about? Give me a brief 2-3 sentence summary.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": long_prompt}]\n",
    "    \n",
    "    print(\"Testing Gemini caching with long prompt...\")\n",
    "    print(f\"Prompt length: {len(long_prompt)} characters\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test 1: First call (should not have cache hits)\n",
    "    print(\"\\n🔥 FIRST CALL (no cache):\")\n",
    "    response1 = await llm.query(\n",
    "        model_name=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"Response: {response1.content}\")\n",
    "    print(f\"Usage: {response1.usage}\")\n",
    "    print(f\"Cost: ${response1.cost:.6f}\")\n",
    "    time.sleep(10)\n",
    "    # Test 2: Second call with previous response + new question (should have cache hits if caching works)\n",
    "    print(\"\\n♻️  SECOND CALL (with previous response + new question):\")\n",
    "    \n",
    "    # Create new messages including the previous response and a new question\n",
    "    messages_with_response = [\n",
    "        {\"role\": \"user\", \"content\": long_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response1.content},\n",
    "        {\"role\": \"user\", \"content\": \"Based on your summary, can you now tell me what are the 3 most important features of a good socratic conversation?\"}\n",
    "    ]\n",
    "    \n",
    "    response2 = await llm.query(\n",
    "        model_name=MODEL_NAME, \n",
    "        messages=messages_with_response,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"Response: {response2.content}\")\n",
    "    print(f\"Usage: {response2.usage}\")\n",
    "    print(f\"Cost: ${response2.cost:.6f}\")\n",
    "    \n",
    "    # Test 3: Streaming version with same conversation context\n",
    "    print(\"\\n🌊 STREAMING CALL (with conversation context):\")\n",
    "    messages_with_response2 = [\n",
    "        {\"role\": \"user\", \"content\": long_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response1.content},\n",
    "        {\"role\": \"user\", \"content\": \"Based on your summary, can you now tell me what are the 3 most important features of a good socratic conversation?\"},\n",
    "        {\"role\": \"assistant\", \"content\": response2.content},\n",
    "        {\"role\": \"user\", \"content\": \"What is the most difficult part of a socratic conversation in your opinion?\"}\n",
    "    ]\n",
    "    stream = await llm.query(\n",
    "        model_name=MODEL_NAME,\n",
    "        messages=messages_with_response2, \n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    full_content = \"\"\n",
    "    async for chunk in stream:\n",
    "        full_content += chunk\n",
    "    \n",
    "    provider = llm._get_provider(MODEL_NAME)\n",
    "    if hasattr(provider, 'last_usage') and provider.last_usage:\n",
    "        print(f\"Streamed response: {full_content}\")\n",
    "        print(f\"Usage: {provider.last_usage}\")\n",
    "        try:\n",
    "            from llm_master.classes import ModelRegistry\n",
    "            model_config = ModelRegistry.get_config(MODEL_NAME)\n",
    "            cost = provider.last_usage.calculate_cost(model_config)\n",
    "            print(f\"Cost: ${cost:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cost calculation failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🔍 ANALYSIS:\")\n",
    "    print(f\"Call 1 tokens: {response1.usage.input_tokens} input, {response1.usage.output_tokens} output, {response1.usage.cached_tokens} cached\")\n",
    "    print(f\"Call 2 tokens: {response2.usage.input_tokens} input, {response2.usage.output_tokens} output, {response2.usage.cached_tokens} cached\")\n",
    "    if hasattr(provider, 'last_usage') and provider.last_usage:\n",
    "        print(f\"Stream tokens: {provider.last_usage.input_tokens} input, {provider.last_usage.output_tokens} output, {provider.last_usage.cached_tokens} cached\")\n",
    "    \n",
    "    if response2.usage.cached_tokens > 0:\n",
    "        print(\"🎉 CACHING IS WORKING!\")\n",
    "    else:\n",
    "        print(\"❌ No cache hits detected\")\n",
    "\n",
    "# Run the test\n",
    "await test_gemini_caching_with_long_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o3\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"search_context_size\": \"low\",\n",
    "    }],\n",
    "    input=\"What movie won best picture in 2025?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uflo AI Server",
   "language": "python",
   "name": "ai-server"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
